{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple Example  of a Gradient Descent Method  by Hand (batch  method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ibrahim\n",
    "## Sobh\n",
    "## PS: Please add your name to the file name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math as mt\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn.datasets import make_regression \n",
    "import pylab\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We're going to solve a  Least Squares Problem with a cost function defined as following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$J(\\Theta)=\\frac{1}{2*m} \\sum_{i=1}^{m} (H_\\Theta(x_i)−y_i)^2$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$$J(\\Theta)=\\frac{1}{2*m} \\sum_{i=1}^{m} (H_\\Theta(x_i)−y_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# where \n",
    "### i) x_i is the ith sample\n",
    "### ii) m is the total number of training examples \n",
    "#### and Hθ(x(i)) is the hypothesis function defined like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$h_\\Theta(x_i)=\\Theta_0  + \\Theta_1 x_i$$\n",
       "###### Constant (intercept) and Slope\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$$h_\\Theta(x_i)=\\Theta_0  + \\Theta_1 x_i$$\n",
    "###### Constant (intercept) and Slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Scipy/Numpy/Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.62 ms, sys: 6.09 ms, total: 15.7 ms\n",
      "Wall time: 17.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Generation of data: 1st method\n",
    "n=10**6\n",
    "X=np.linspace(0,1,n)\n",
    "y=1 + X + X*np.random.random(len(X))/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"X: \",X, \"Shape: \", np.shape(X))\n",
    "print(\"y: \",y, \"Shape: \", np.shape(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F(x,a,b):\n",
    "    return a*x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.04984713 1.00004062]\n",
      "CPU times: user 58 ms, sys: 23.6 ms, total: 81.7 ms\n",
      "Wall time: 106 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# We solve with Scipy: optimize\n",
    "Coeffs=optimize.curve_fit(F,xdata=X,ydata=y)[0]\n",
    "print(Coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We solve using numpy.linalg.lstsqe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (1000000, 2)\n",
      "Shape:  (1000000, 1)\n",
      "CPU times: user 3.88 ms, sys: 10.8 ms, total: 14.7 ms\n",
      "Wall time: 19.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# We construct the Matrix A\n",
    "A=np.vstack([X,np.ones(len(X))]).T\n",
    "print(\"Shape: \", np.shape(A))\n",
    "Y=y[:,np.newaxis]\n",
    "print(\"Shape: \", np.shape(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.04984713]\n",
      " [1.00004062]]\n",
      "CPU times: user 57.8 ms, sys: 39.4 ms, total: 97.2 ms\n",
      "Wall time: 25.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# We solve using numpy.linalg.lstsqe\n",
    "Coeffs=np.linalg.lstsq(A,Y,rcond=None)[0]\n",
    "print(Coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 193 ms, sys: 83.7 ms, total: 277 ms\n",
      "Wall time: 52.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Using the Pseudo_Inverse\n",
    "Apinv=np.linalg.pinv(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hum:  [[1.04984713]\n",
      " [1.00004062]]\n",
      "CPU times: user 6.02 ms, sys: 2.47 ms, total: 8.49 ms\n",
      "Wall time: 4.11 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Coeffs=Apinv.dot(Y)\n",
    "print(\"Hum: \", Coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope: 1.0498471275310675\n",
      "Intercept:  1.0000406230804677\n",
      "r_value, p_value, slope_std_error:  0.9984920396435913 0.0 5.772023163040131e-05\n",
      "CPU times: user 13.6 ms, sys: 2.45 ms, total: 16 ms\n",
      "Wall time: 12.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# We check a good approximation of the \"exact\" value\n",
    "# with scipy linear regression \n",
    "slope, intercept, r_value, p_value, slope_std_error = stats.linregress(X, y)\n",
    "print(\"Slope:\", slope)\n",
    "print(\"Intercept: \",intercept)\n",
    "print(\"r_value, p_value, slope_std_error: \", r_value, p_value, slope_std_error )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.000000e+00 1.000001e-06 2.000002e-06 ... 9.999980e-01 9.999990e-01\n",
      " 1.000000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]\n",
      " [2 2]\n",
      " [3 3]\n",
      " [4 4]]\n",
      "[ 3.3  6.3  9.3 12.3]\n"
     ]
    }
   ],
   "source": [
    "# With Scikit: be careful of the SHAPEs!\n",
    "from sklearn.linear_model  import LinearRegression\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(X,y,random_state=42)\n",
    "Xx = np.array([[1, 1], [2, 2], [3, 3], [4, 4]])\n",
    "print(Xx)\n",
    "# y = 1 * x_0 + 2 * x_1 + 3\n",
    "yy = np.dot(Xx, np.array([1, 2])) + 0.3\n",
    "print(yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LinearRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kq/4phtjsxn40794zp8lvjfsccw0000gn/T/ipykernel_46541/3652811263.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LinearRegression' is not defined"
     ]
    }
   ],
   "source": [
    "reg = LinearRegression().fit(Xx, yy)\n",
    "print(reg.score(Xx, yy))\n",
    "print(reg.coef_)\n",
    "print(reg.intercept_)\n",
    "print(reg.predict(np.array([[3, 5]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TRY WITH 10**7 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A \"Gradient Code\"...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha is the Learning rate\n",
    "# x Set of x values\n",
    "# y Set of y values\n",
    "# eps\n",
    "def Gradient_Method(alpha, \n",
    "                    x, \n",
    "                    y, \n",
    "                    eps=0.0001, \n",
    "                    max_iter=1000, \n",
    "                    steps=10):\n",
    "    HasConverged = False\n",
    "    iter = 0\n",
    "    m = x.shape[0] # number of samples\n",
    "    print(\"Number of Samples:\", m)\n",
    "\n",
    "    # initial value for theta  : chosen randomly\n",
    "    # Theta=(Theta0[0],Theta1[0]) in IR^2\n",
    "    # Theta0[0] -> t0 in IR\n",
    "    # Theta1[0] -> t1 in IR\n",
    "    \n",
    "    #Pick a random Intercept value\n",
    "    t0 = np.random.random(x.shape[1])\n",
    "    \n",
    "    #pick a random Slope value\n",
    "    t1 = np.random.random(x.shape[1])\n",
    "    \n",
    "    print(\"Initial Value of Thetao[0] =\", t0)\n",
    "    \n",
    "    print(\"Initial Value of Theta1[0] =\", t1)\n",
    "\n",
    "    # The total error, J(theta)  is defined as\n",
    "    \n",
    "    # Loss function = Sigma[(T1*x[i] + T0) - Y(i))^2]\n",
    "    \n",
    "    # Y[i] ---------->Ovserved Y \n",
    "    \n",
    "    # T1*x[i] + T0 ------> Predicited Y\n",
    "    \n",
    "    # J is the Loss function ( in this case it is Least Square )\n",
    "    # This is the intial Erro value\n",
    "      \n",
    "    J= sum( [ (t0 + t1*x[i] - y[i])**2 for i in range(m)] ) \n",
    "    \n",
    "    print(\"Total Initial Error J: \", J)\n",
    "\n",
    "    # The Iteration Loop\n",
    "    while not HasConverged:\n",
    "        # for each training sample, compute the gradient (d/d_theta j(theta))\n",
    "        # following: \n",
    "        \n",
    "        # Derivative (loss Function)\n",
    "        grad0 = 1.0/m * sum([(t0 + t1*x[i] - y[i]) for i in range(m)]) \n",
    "        grad1 = 1.0/m * sum([(t0 + t1*x[i] - y[i])*x[i] for i in range(m)])\n",
    "\n",
    "        # We update the theta_temp\n",
    "        temp0 = t0 - alpha * grad0\n",
    "        temp1 = t1 - alpha * grad1\n",
    "        \n",
    "    \n",
    "        # We update theta\n",
    "        t0 = temp0\n",
    "        t1 = temp1\n",
    "\n",
    "        # We compute Mean Squared Error\n",
    "        e = sum( [ (t0 + t1*x[i] - y[i])**2 for i in range(m)] ) \n",
    "\n",
    "        if abs(J-e) <= eps:\n",
    "            print('Converged, iterations: ', iter)\n",
    "            HasConverged = True\n",
    "            \n",
    "        if (iter % steps ==0): print(\"Iter: \", iter, \n",
    "                                     \" Error: \",e, \n",
    "                                     \" J-e:\",abs(J-e), \n",
    "                                     \" t0: \", t0, \n",
    "                                     \" t1: \",t1)    \n",
    "    \n",
    "        J = e      # We update error \n",
    "        iter += 1  # We update iter\n",
    "    \n",
    "        if iter == max_iter:\n",
    "            print('Max interactions exceeded!')\n",
    "            converged = True\n",
    "\n",
    "    return t0,t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = (10000, 1) y.shape = (10000,)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "#   We create the Data     \n",
    "    x, y = make_regression(n_samples=10**4, n_features=1, n_informative=1, \n",
    "                        random_state=0, noise=35) \n",
    "    print('x.shape = %s y.shape = %s' %(x.shape, y.shape))\n",
    "\n",
    "# We choose some hyperparameters  \n",
    "    alpha = 0.01 # learning rate\n",
    "    eps = 0.01 # convergence criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples: 10000\n",
      "Initial Value of Thetao[0] = [0.09888619]\n",
      "Initial Value of Theta1[0] = [0.58669036]\n",
      "Total Initial Error J:  [13002434.6336444]\n",
      "Iter:  0  Error:  [12988106.50170805]  J-e: [14328.13193635]  t0:  [0.09326159]  t1:  [0.67135227]\n",
      "Iter:  100  Error:  [12367385.05048594]  J-e: [2007.88739842]  t0:  [-0.21764273]  t1:  [6.03815115]\n",
      "Iter:  200  Error:  [12280351.86891757]  J-e: [281.81351265]  t0:  [-0.29443429]  t1:  [8.04976629]\n",
      "Iter:  300  Error:  [12268130.05445992]  J-e: [39.61213201]  t0:  [-0.30866785]  t1:  [8.80404593]\n",
      "Iter:  400  Error:  [12266411.27458183]  J-e: [5.57582635]  t0:  [-0.30867442]  t1:  [9.08697275]\n",
      "Iter:  500  Error:  [12266169.22233916]  J-e: [0.7859144]  t0:  [-0.30672472]  t1:  [9.19313408]\n",
      "Iter:  600  Error:  [12266135.08949054]  J-e: [0.11091661]  t0:  [-0.30527846]  t1:  [9.23298198]\n",
      "Iter:  700  Error:  [12266130.2702252]  J-e: [0.01567266]  t0:  [-0.30447405]  t1:  [9.24794392]\n",
      "Converged, iterations:  723\n",
      "Computed value\n",
      "Theta0 = [-0.30435503]       Theta1 = [9.24975881]\n",
      "Intercept = -0.3037486491107916 Slope = 9.256946612556051\n",
      "Error on Slope [0.0071878]\n",
      "Error on Intercept [0.00060638]\n",
      "CPU times: user 57.4 s, sys: 131 ms, total: 57.5 s\n",
      "Wall time: 57.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# We call gradient descent, this gives approximation \n",
    "# of theta0 and theta1\n",
    "theta0, theta1 = Gradient_Method(alpha, x, y, eps, max_iter=1000,\n",
    "                                     steps=100)\n",
    "print(\"Computed value\")\n",
    "print(('Theta0 = %s       Theta1 = %s') %(theta0, theta1)) \n",
    "\n",
    "# We check a good approximation of the \"exact\" value\n",
    "# with scipy linear regression \n",
    "slope, intercept, r_value, p_value, slope_std_error = stats.linregress(x[:,0], y)\n",
    "print(('Intercept = %s Slope = %s') %(intercept, slope))\n",
    "print(\"Error on Slope\", slope-theta1)\n",
    "print(\"Error on Intercept\", intercept-theta0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well Done!\n"
     ]
    }
   ],
   "source": [
    "# When n is not to large ....\n",
    "# We plot  the data and the line we have computed, i.e.: Y= Theta0 + Theta1 * X\n",
    "for i in range(x.shape[0]):\n",
    "        y_predict = theta0 + theta1*x \n",
    "\n",
    "pylab.plot(x,y,'o')\n",
    "pylab.plot(x,y_predict,'k-')\n",
    "pylab.show()\n",
    "print(\"Well Done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do some tests with n=10^3, 10^4, 10^5, 10^6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# We call gradient descent, this gives approximation \n",
    "# of theta0 and theta1\n",
    "# WE CHANGE the value of \"alpha\"\n",
    "alpha=0.025\n",
    "theta0, theta1 = Gradient_Method(alpha, x, y, eps, max_iter=1000,\n",
    "                                     steps=100)\n",
    "print(\"Computed value\")\n",
    "print(('Theta0 = %s       Theta1 = %s') %(theta0, theta1)) \n",
    "\n",
    "# We check a good approximation of the \"exact\" value\n",
    "# with scipy linear regression \n",
    "slope, intercept, r_value, p_value, slope_std_error = stats.linregress(x[:,0], y)\n",
    "print(('Intercept = %s Slope = %s') %(intercept, slope))\n",
    "print(\"Error on Slope\", slope-theta1)\n",
    "print(\"Error on Intercept\", intercept-theta0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples: 10000\n",
      "Initial Value of Thetao[0] = [0.2474708]\n",
      "Initial Value of Theta1[0] = [0.0300796]\n",
      "Total Initial Error J:  [13101627.40635273]\n",
      "Iter:  0  Error:  [13045392.29154858]  J-e: [56235.11480415]  t0:  [0.22222513]  t1:  [0.34549835]\n",
      "Iter:  100  Error:  [12266870.6294179]  J-e: [53.33075472]  t0:  [-0.30646971]  t1:  [8.98128546]\n",
      "Iter:  200  Error:  [12266130.1942417]  J-e: [0.05150981]  t0:  [-0.30437267]  t1:  [9.24838182]\n",
      "Converged, iterations:  224\n",
      "Computed value\n",
      "Theta0 = [-0.3040731]       Theta1 = [9.25322146]\n",
      "Intercept = -0.3037486491107916 Slope = 9.256946612556051\n",
      "Error on Slope [0.00372516]\n",
      "Error on Intercept [0.00032445]\n",
      "CPU times: user 19.6 s, sys: 38.9 ms, total: 19.7 s\n",
      "Wall time: 19.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# We call gradient descent, this gives approximation \n",
    "# of theta0 and theta1\n",
    "# WE CHANGE the value of \"alpha\"\n",
    "alpha=0.035\n",
    "theta0, theta1 = Gradient_Method(alpha, x, y, eps, max_iter=1000,\n",
    "                                     steps=100)\n",
    "print(\"Computed value\")\n",
    "print(('Theta0 = %s       Theta1 = %s') %(theta0, theta1)) \n",
    "21\n",
    "# We check a good approximation of the \"exact\" value\n",
    "# with scipy linear regression \n",
    "slope, intercept, r_value, p_value, slope_std_error = stats.linregress(x[:,0], y)\n",
    "print(('Intercept = %s Slope = %s') %(intercept, slope))\n",
    "print(\"Error on Slope\", slope-theta1)\n",
    "print(\"Error on Intercept\", intercept-theta0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some remarks\n",
    "#### I have chosen the function  Gradient_Method because: \n",
    "#### well, it is not really well written, so it can be improved\n",
    "#### First the initial value of  Theta0 and Theta1 are chosen strictly randomly, which is the worst choice we can do. We can do a better choice.\n",
    "#### Second, the \"gradient\"  is computed with the full dataset (i.e. the full set of avalaible data), which has a huge cost again, we can do better, at least initially. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea: when you will have your code, \n",
    "## Try different (and growing) values of m (number of samples). And use %%time to compare\n",
    "## Example: m=100, m=1000, m=5000, m=10000, m=10^5, m=10^6 (be careful, it can takes a longggggg time!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## PROBLEM 1\n",
    "#### Here we choose really randomly the inital  value of Theta0 and Theta1, they are ### what we call Theta0[0] and Theta1[0] in the classical iterative algorithms \n",
    "#### (Theta0[i+1] ,Theta1[i+1])=(Theta0[i] ,Theta1[i]) - Lambda * Grad(F(Theta0[i] ,Theta1[i]))\n",
    "#### You are asked to:\n",
    "#### (i) Propose another method, fast, efficient and usefull (fast! i.e. with a low complexity)\n",
    "#### (ii) Implement it, test it, justify it (here)\n",
    "#### (iii) Please call your function Gradient_Init_Prob1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1 of my answer\n",
    "## ( Keeping the same Function With Some tweaks to the intiale Theta 0 , Theta 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_initial_thetas(x,y):\n",
    "    \n",
    "    ''' Trying to find the average Intercept (Theta 0)/ that has the steapest line Slope (Theta 1)'''\n",
    "    ''' Calcuating the highest Slope with the intercept using a bunch of point on the Up Far Right '''\n",
    "\n",
    "    theta0=np.mean(y)\n",
    "    \n",
    "    index_x_min= np.argmin(x)\n",
    "    x_mask= (x >= np.mean(x)) & (x <= np.amax(x))\n",
    "    x_batch= x[x_mask]\n",
    "    itemindex = np.where(x_batch)\n",
    "    y_batch= y[itemindex]\n",
    "    \n",
    "    x_batch=x_batch[itemindex]\n",
    "    y_batch=y_batch[itemindex]\n",
    "    \n",
    "    if len(y)>10:\n",
    "        x_batch=x_batch[len(x_batch)-2:-1]\n",
    "        y_batch=y_batch[len(y_batch)-2:-1]\n",
    "\n",
    "    X=x_batch\n",
    "    Y=y_batch\n",
    "    \n",
    "    T=(Y-y[index_x_min])/(X- x[index_x_min])\n",
    "    theta1=np.amin(T[T>0])\n",
    "    \n",
    "    return theta0,theta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha is the Learning rate\n",
    "# x Set of x values\n",
    "# y Set of y values\n",
    "# eps\n",
    "def Gradient_Init_Prob1_Version_1(alpha, \n",
    "                    x, \n",
    "                    y, \n",
    "                    eps=0.0001, \n",
    "                    max_iter=1000, \n",
    "                    steps=10):\n",
    "    HasConverged = False\n",
    "    iter = 0\n",
    "    m = x.shape[0] # number of samples\n",
    "    print(\"Number of Samples:\", m)\n",
    "\n",
    "    # initial value for theta  : chosen randomly\n",
    "    # Theta=(Theta0[0],Theta1[0]) in IR^2\n",
    "    # Theta0[0] -> t0 in IR\n",
    "    # Theta1[0] -> t1 in IR\n",
    "    \n",
    "    #Pick a random Intercept value\n",
    "    #pick a random Slope value\n",
    "    \n",
    "    t0,t1=get_initial_thetas(x,y)\n",
    "    \n",
    "    \n",
    "    print(\"Initial Value of Thetao[0] =\", t0)\n",
    "    \n",
    "    print(\"Initial Value of Theta1[0] =\", t1)\n",
    "\n",
    "    # The total error, J(theta)  is defined as\n",
    "    \n",
    "    # Loss function = Sigma[(T1*x[i] + T0) - Y(i))^2]\n",
    "    \n",
    "    # Y[i] ---------->Ovserved Y \n",
    "    \n",
    "    # T1*x[i] + T0 ------> Predicited Y\n",
    "    \n",
    "    # J is the Loss function ( in this case it is Least Square )\n",
    "    # This is the intial Erro value\n",
    "      \n",
    "    J= sum( [ (t0 + t1*x[i] - y[i])**2 for i in range(m)] ) \n",
    "    \n",
    "    print(\"Total Initial Error J: \", J)\n",
    "\n",
    "    # The Iteration Loop\n",
    "    while not HasConverged:\n",
    "        # for each training sample, compute the gradient (d/d_theta j(theta))\n",
    "        # following: \n",
    "        \n",
    "        # Derivative (loss Function)\n",
    "        grad0 = 1.0/m * sum([(t0 + t1*x[i] - y[i]) for i in range(m)]) \n",
    "        grad1 = 1.0/m * sum([(t0 + t1*x[i] - y[i])*x[i] for i in range(m)])\n",
    "    \n",
    "        # We update the theta_temp\n",
    "        temp0 = t0 - alpha * grad0\n",
    "        temp1 = t1 - alpha * grad1\n",
    "        \n",
    "    \n",
    "        # We update theta\n",
    "        t0 = temp0\n",
    "        t1 = temp1\n",
    "\n",
    "        # We compute Mean Squared Error\n",
    "        e = sum( [ (t0 + t1*x[i] - y[i])**2 for i in range(m)] ) \n",
    "\n",
    "        if abs(J-e) <= eps:\n",
    "            print('Converged, iterations: ', iter)\n",
    "            HasConverged = True\n",
    "            \n",
    "        if (iter % steps ==0): print(\"Iter: \", iter, \n",
    "                                     \" Error: \",e, \n",
    "                                     \" J-e:\",abs(J-e), \n",
    "                                     \" t0: \", t0, \n",
    "                                     \" t1: \",t1)    \n",
    "    \n",
    "        J = e      # We update error \n",
    "        iter += 1  # We update iter\n",
    "    \n",
    "        if iter == max_iter:\n",
    "            print('Max interactions exceeded!')\n",
    "            converged = True\n",
    "\n",
    "    return t0,t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2 (Numpy Version)  ---> Much FASTER!!\n",
    "## ( Rebuild the Gradient function Using \"Numpy\" instead of List Comprehnsion ) \n",
    "## With Some tweaks to the intiale Theta 0 , Theta 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_grad(x,y,thetas_hist):\n",
    "    if int(mt.log10(x.shape[0])) > 4:\n",
    "         print (\"\\n---->Plot takes forever try wiht a smaller number to get a nice Graph Representation :)\\n\")\n",
    "    else:\n",
    "        iteration=0\n",
    "        for theta in thetas_hist:\n",
    "            theta0=theta[0][0]\n",
    "            theta1=theta[0][1]\n",
    "            if iteration==0:\n",
    "                pylab.plot(x,y,'o')\n",
    "            if iteration%16==0:\n",
    "                for i in range(x.shape[0]):\n",
    "                    y_predict = theta0 + theta1*x \n",
    "                pylab.plot(x,y_predict,'k--')\n",
    "            if iteration==len(thetas_hist)-1:\n",
    "                for i in range(x.shape[0]):\n",
    "                    y_predict = theta0 + theta1*x \n",
    "                pylab.plot(x,y_predict,'r-')\n",
    "            iteration+=1\n",
    "        pylab.show()\n",
    "    \n",
    "\n",
    "def cost(theta,X,y):\n",
    "    '''\n",
    "    Calculates cost of the function.\n",
    "    X & y are our points where X is (2,N).\n",
    "    theta - vector.\n",
    "    '''\n",
    "    m = len(y)\n",
    "    # Calculating Cost\n",
    "    cost_value =  np.sum(np.square((theta.dot(X))-y))  \n",
    "    print(cost_value)\n",
    "    return cost_value\n",
    "\n",
    "def Gradient_Init_Prob1_Version_2(learning_rate, \n",
    "                    x, \n",
    "                    y, \n",
    "                    eps=0.0001, \n",
    "                    max_iter=1000, \n",
    "                    steps=10):\n",
    "    \n",
    "            X_cost=x\n",
    "            X=x.reshape((-1,))\n",
    "            Y=y\n",
    "            \n",
    "            print(\"size of input: \", x.shape[0])\n",
    "\n",
    "            # Creating an array of ones for algebraic calculation\n",
    "            ones = [1] * len(X)\n",
    "\n",
    "            # Here array of ones will be concatenated with X and transpose will be perform\n",
    "            # This signifies that matrix multiplication can help to obtain new value\n",
    "\n",
    "            # of theta 0 and theta 1\n",
    "            X = np.transpose(np.concatenate((np.array([ones]).reshape(-1, 1),\n",
    "            np.array([X]).reshape(-1, 1)), axis=1))\n",
    "\n",
    "            # [[1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
    "            # [value1, value2 , ....................value_ n]]\n",
    "\n",
    "            # Same declaring starting value of two thetas, theta 0 and theta 1,\n",
    "            \n",
    "            #theta = np.array([[np.mean(y),np.mean(y)]])\n",
    "            theta0,theta1=get_initial_thetas(x,y)\n",
    "            theta=np.array([[theta0,theta1]])\n",
    "            print(\"Initial Value of Thetao[0] =\", theta0)\n",
    "            print(\"Initial Value of Theta1[0] =\", theta1)\n",
    "            intital_cost=cost(theta,X,y)\n",
    "            print('Total Initial Cost:',round(cost(theta,X,y),5))\n",
    "            \n",
    "            theta_hist = []\n",
    "            for iter in range(max_iter):\n",
    "                htheta = np.dot(theta, X)\n",
    "                diff_theta = htheta - Y\n",
    "                partial_derivative_theta = np.dot(diff_theta, np.transpose(X)) / len(Y)\n",
    "                theta = theta - learning_rate * partial_derivative_theta\n",
    "                new_cost=cost(theta,X,y)\n",
    "                if abs(intital_cost-new_cost) <eps:\n",
    "                    print('Converged, iterations: ', iter)\n",
    "                    break\n",
    "                \n",
    "                theta_hist.append(theta)\n",
    "                theta0 = theta_hist[-1][0][0]\n",
    "                theta1 = theta_hist[-1][0][1]\n",
    "                if (iter % steps ==0): \n",
    "                    print(\"Iter: \", iter, \n",
    "                             \" Error: \",round(new_cost,5), \n",
    "                             \" [Old-New](Cost):\",round(abs(intital_cost-new_cost),5), \n",
    "                             \" t0: \", theta0, \n",
    "                             \" t1: \",theta1)\n",
    "\n",
    "                intital_cost=new_cost\n",
    "                \n",
    "            if iter == max_iter:\n",
    "                print('Max interactions exceeded!')\n",
    "                \n",
    "            return theta0,theta1,theta_hist\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate X,Y Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_regression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kq/4phtjsxn40794zp8lvjfsccw0000gn/T/ipykernel_46541/2546057289.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#   We create the Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     x, y = make_regression(n_samples=10**4, n_features=1, n_informative=1, \n\u001b[0m\u001b[1;32m      5\u001b[0m                         random_state=0, noise=35) \n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x.shape = %s y.shape = %s'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_regression' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "#   We create the Data     \n",
    "    x, y = make_regression(n_samples=10**4, n_features=1, n_informative=1, \n",
    "                        random_state=0, noise=35) \n",
    "    print('x.shape = %s y.shape = %s' %(x.shape, y.shape))\n",
    "\n",
    "# We choose some hyperparameters  \n",
    "    alpha = 0.01 # learning rate\n",
    "    eps = 0.01 # convergence criteri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples: 10000\n",
      "Initial Value of Thetao[0] = -0.47438861248665654\n",
      "Initial Value of Theta1[0] = 10.132564648213513\n",
      "Total Initial Error J:  [12273955.79484907]\n",
      "Iter:  0  Error:  [12273426.058739]  J-e: [529.73611007]  t0:  [-0.46785128]  t1:  [10.10255546]\n",
      "Iter:  100  Error:  [12266136.15468118]  J-e: [0.48311054]  t0:  [-0.30673653]  t1:  [9.28287769]\n",
      "Converged, iterations:  156\n",
      "Computed value\n",
      "Theta0 = [-0.30401995]       Theta1 = [9.26063911]\n",
      "Intercept = -0.3037486491107916 Slope = 9.256946612556051\n",
      "Error on Slope [-0.0036925]\n",
      "Error on Intercept [0.0002713]\n",
      "CPU times: user 12.7 s, sys: 42.7 ms, total: 12.7 s\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# We call gradient descent, this gives approximation \n",
    "# of theta0 and theta1\n",
    "# WE CHANGE the value of \"alpha\"\n",
    "#alpha=0.035\n",
    "alpha=0.035\n",
    "theta0, theta1 = Gradient_Init_Prob1_Version_1(alpha, x, y, eps, max_iter=1000,\n",
    "                                     steps=100)\n",
    "print(\"Computed value\")\n",
    "print(('Theta0 = %s       Theta1 = %s') %(theta0, theta1)) \n",
    "21\n",
    "# We check a good approximation of the \"exact\" value\n",
    "# with scipy linear regression \n",
    "slope, intercept, r_value, p_value, slope_std_error = stats.linregress(x[:,0], y)\n",
    "print(('Intercept = %s Slope = %s') %(intercept, slope))\n",
    "print(\"Error on Slope\", slope-theta1)\n",
    "print(\"Error on Intercept\", intercept-theta0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# We call gradient descent, this gives approximation \n",
    "# of theta0 and theta1\n",
    "# WE CHANGE the value of \"alpha\"\n",
    "#alpha=0.035\n",
    "alpha=0.035\n",
    "theta0, theta1,thetas_hist = Gradient_Init_Prob1_Version_2(alpha, x, y, eps, max_iter=1000,\n",
    "                                     steps=100)\n",
    "print(\"Computed value\")\n",
    "print(('Theta0 = %s       Theta1 = %s') %(theta0, theta1)) \n",
    "21\n",
    "# We check a good approximation of the \"exact\" value\n",
    "# with scipy linear regression \n",
    "slope, intercept, r_value, p_value, slope_std_error = stats.linregress(x[:,0], y)\n",
    "print(('Intercept = %s Slope = %s') %(intercept, slope))\n",
    "print(\"Error on Slope\", slope-theta1)\n",
    "print(\"Error on Intercept\", intercept-theta0)\n",
    "plot_grad(x,y,thetas_hist)\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# justification\n",
    "\n",
    "\n",
    "Version 1\n",
    "----------\n",
    "Trying to find the average Intercept (Theta 0)/ that has the steapest line Slope (Theta 1)\n",
    "Calcuating the highest Slope with the intercept using a bunch of point on the Up Far Right\n",
    "\n",
    "where Slope = Minimum positive Slope =min ( ( X-x )/ (Y-y))\n",
    "where Intercept = y average\n",
    "using this technique will reduce the number of iterations with need\n",
    "\n",
    "Version 2\n",
    "----------\n",
    "\n",
    "General Info\n",
    "------------\n",
    "NumPy\n",
    "------\n",
    "Provides support for large multidimensional arrays and matrices along with a collection of mathematical functions to operate on these elements. \n",
    "Python\n",
    "------\n",
    "While it is technically possible to implement scalar and matrix calculations using Python lists. However, this can be unwieldy, and performance is poor when compared to languages suited for numerical computations\n",
    "\n",
    "Thus Dealing with this problem requires some fast computations\n",
    "Using this technique can fasten the time to get results drastically\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Cost =  \\sum( \\Theta.X− Y)^2 \\newline $$\n",
    "\n",
    "$$Partial\\space Derivative\\space\\Theta = \\frac{1}m * (\\Theta X - Y)X^T $$\n",
    "\n",
    "$$where\\space \\Theta, X, Y\\space\\space  are\\space vectors\\space and\\space X\\space is\\space of\\space the\\space form\\space\n",
    "(2,N)\\space with\\space first\\space column\\space have\\space N\\space ones\\space (1)\\space$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROBLEM 2\n",
    "#### Here we \"upgdrade\", in the function Gradient_Method, the gradient is using the \"full batch\", i.e. the full set of avalaible data. You are asked to:\n",
    "#### (i) Implement  the \"minibatch\" idea.\n",
    "#### (ii) Test it. Here . Use %%time to comapre CPU time for \"convergence\" \n",
    "#### #### PS: Please call your function Gradient_Method_Prob2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Idea: when you will have your code, \n",
    "## Try different (and growing) values of m (number of samples). And use %%time to compare\n",
    "## Example: m=100, m=1000, m=5000, m=10000, m=10^5, m=10p^6 \n",
    "## And, NOW, if possible: m=10^7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Slice_Batch(x,y):\n",
    "    full_size_power=int(mt.log10(x.shape[0]))\n",
    "    if full_size_power>3:\n",
    "        limit=2500*mt.ceil(full_size_power/2 +1)# Batch_size\n",
    "        x=np.concatenate([x[0:limit//2], # First sub-batch At the begning until 2 times limit\n",
    "                          x[len(x)//2-limit//2:len(x)//2],# Second sub-batch At the begning (middle - limit)\n",
    "                          x[len(x)//2: len(x)//2 + limit//2],# Third sub-batch At the begning (middle + limit)\n",
    "                          x[len(x)-limit//2:]])# Fourth sub-batch At the begning until limit\n",
    "        \n",
    "        y=np.concatenate([y[0:limit//2], # First sub-batch At the begning until 2 times limit\n",
    "                          y[len(y)//2-limit//2:len(y)//2],# Second sub-batch At the begning (middle - limit)\n",
    "                          y[len(y)//2:len(y)//2+limit//2],# Third sub-batch At the begning (middle + limit)\n",
    "                          y[len(y)-limit//2:]])# Fourth sub-batch At the begning until limit\n",
    "        print(\"Size of Batch: \", limit)\n",
    "\n",
    "    else:\n",
    "        print(\"mini - batch is not necessary due to data set small size \")\n",
    "    return x,y\n",
    "\n",
    "def Gradient_Method_Prob2_Version_1(alpha, \n",
    "                    x, \n",
    "                    y, \n",
    "                    eps=0.0001,\n",
    "                    max_iter=1000, \n",
    "                    steps=10):\n",
    "    HasConverged = False\n",
    "    iter = 0\n",
    "\n",
    "  \n",
    "    print(\"Size of input: \", x.shape[0])\n",
    "    # initial value for theta  : chosen randomly\n",
    "    # Theta=(Theta0[0],Theta1[0]) in IR^2\n",
    "    # Theta0[0] -> t0 in IR\n",
    "    # Theta1[0] -> t1 in IR\n",
    "    \n",
    "    #Pick a random Intercept value\n",
    "    #pick a random Slope value\n",
    "    x,y=Slice_Batch(x,y)\n",
    "    t0,t1=get_initial_thetas(x,y)\n",
    "    \n",
    "    m = x.shape[0] # number of samples\n",
    "    \n",
    "    \n",
    "    print(\"Initial Value of Thetao[0] =\", t0)\n",
    "    \n",
    "    print(\"Initial Value of Theta1[0] =\", t1)\n",
    "\n",
    "    # The total error, J(theta)  is defined as\n",
    "    \n",
    "    # Loss function = Sigma[(T1*x[i] + T0) - Y(i))^2]\n",
    "    \n",
    "    # Y[i] ---------->Ovserved Y \n",
    "    \n",
    "    # T1*x[i] + T0 ------> Predicited Y\n",
    "    \n",
    "    # J is the Loss function ( in this case it is Least Square )\n",
    "    # This is the intial Erro value\n",
    "      \n",
    "    J= sum( [ (t0 + t1*x[i] - y[i])**2 for i in range(m)] ) \n",
    "    \n",
    "    print(\"Total Initial Error J: \", J)\n",
    "\n",
    "    # The Iteration Loop\n",
    "    while not HasConverged:\n",
    "        # for each training sample, compute the gradient (d/d_theta j(theta))\n",
    "        # following: \n",
    "        \n",
    "        # Derivative (loss Function)\n",
    "        grad0 = 1.0/m * sum([(t0 + t1*x[i] - y[i]) for i in range(m)]) \n",
    "        grad1 = 1.0/m * sum([(t0 + t1*x[i] - y[i])*x[i] for i in range(m)])\n",
    "\n",
    "        # We update the theta_temp\n",
    "        temp0 = t0 - alpha * grad0\n",
    "        temp1 = t1 - alpha * grad1\n",
    "        \n",
    "    \n",
    "        # We update theta\n",
    "        t0 = temp0\n",
    "        t1 = temp1\n",
    "\n",
    "        # We compute Mean Squared Error\n",
    "        e = sum( [ (t0 + t1*x[i] - y[i])**2 for i in range(m)] ) \n",
    "\n",
    "        if abs(J-e) <= eps:\n",
    "            print('Converged, iterations: ', iter)\n",
    "            HasConverged = True\n",
    "            \n",
    "        if (iter % steps ==0): print(\"Iter: \", iter, \n",
    "                                     \" Error: \",e, \n",
    "                                     \" J-e:\",abs(J-e), \n",
    "                                     \" t0: \", t0, \n",
    "                                     \" t1: \",t1)    \n",
    "    \n",
    "        J = e      # We update error \n",
    "        iter += 1  # We update iter\n",
    "    \n",
    "        if iter == max_iter:\n",
    "            print('Max interactions exceeded!')\n",
    "            converged = True\n",
    "\n",
    "    return t0,t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2 (Numpy Version) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gradient_Method_Prob2_Version_2(learning_rate, \n",
    "                    x, \n",
    "                    y, \n",
    "                    eps=0.0001, \n",
    "                    max_iter=1000, \n",
    "                    steps=10):\n",
    "            print(\"Size of input: \", x.shape[0])\n",
    "            \n",
    "            x,y=Slice_Batch(x,y)\n",
    "            \n",
    "            X=x.reshape((-1,))\n",
    "            Y=y\n",
    "            \n",
    "\n",
    "            # Creating an array of ones for algebraic calculation\n",
    "            ones = [1] * len(X)\n",
    "\n",
    "            # Here array of ones will be concatenated with X and transpose will be perform\n",
    "            # This signifies that matrix multiplication can help to obtain new value\n",
    "\n",
    "            # of theta 0 and theta 1\n",
    "            X = np.transpose(np.concatenate((np.array([ones]).reshape(-1, 1),\n",
    "            np.array([X]).reshape(-1, 1)), axis=1))\n",
    "\n",
    "            # [[1.  1.  1.  1.  1.  1.  1.  1.  1.  1. ]\n",
    "            # [value1, value2 , ....................value_ n]]\n",
    "\n",
    "            # Same declaring starting value of two thetas, theta 0 and theta 1,\n",
    "            \n",
    "            #theta = np.array([[np.mean(y),np.mean(y)]])\n",
    "            theta0,theta1=get_initial_thetas(x,y)\n",
    "            \n",
    "            theta=np.array([[theta0,theta1]])\n",
    "            \n",
    "            print(\"Initial Value of Thetao[0] =\", theta0)\n",
    "            print(\"Initial Value of Theta1[0] =\", theta1)\n",
    "            intital_cost=cost(theta,X,y)\n",
    "            print('Total Initial Cost',round(cost(theta,X,y),5))\n",
    "            \n",
    "            theta_hist = []\n",
    "            for iter in range(max_iter):\n",
    "                htheta = np.dot(theta, X)\n",
    "                diff_theta = htheta - Y\n",
    "                partial_derivative_theta = np.dot(diff_theta, np.transpose(X)) / len(Y)\n",
    "                theta = theta - learning_rate * partial_derivative_theta\n",
    "                new_cost=cost(theta,X,y)\n",
    "                if abs(intital_cost-new_cost) <eps:\n",
    "                    print('Converged, iterations: ', iter)\n",
    "                    break\n",
    "                \n",
    "                theta_hist.append(theta)\n",
    "                theta0 = theta_hist[-1][0][0]\n",
    "                theta1 = theta_hist[-1][0][1]\n",
    "                if (iter % steps ==0): \n",
    "                    print(\"Iter: \", iter, \n",
    "                             \" Error: \",round(new_cost,5), \n",
    "                             \" J-e:\",round(abs(intital_cost-new_cost),5), \n",
    "                             \" t0: \", theta0, \n",
    "                             \" t1: \",theta1)\n",
    "\n",
    "                intital_cost=new_cost\n",
    "                \n",
    "            if iter == max_iter:\n",
    "                print('Max interactions exceeded!')\n",
    "                \n",
    "            return theta0,theta1,theta_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = (1000000, 1) y.shape = (1000000,)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "#   We create the Data     \n",
    "    x, y = make_regression(n_samples=10**6, n_features=1, n_informative=1, \n",
    "                        random_state=0, noise=35) \n",
    "    print('x.shape = %s y.shape = %s' %(x.shape, y.shape))\n",
    "\n",
    "# We choose some hyperparameters  \n",
    "    alpha = 0.01 # learning rate\n",
    "    eps = 0.01 # convergence criteri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of input:  1000000\n",
      "Size of Batch:  10000\n",
      "Initial Value of Thetao[0] = -0.03790922883399471\n",
      "Initial Value of Theta1[0] = 55.77006281828385\n",
      "Total Initial Error J:  [37279631.04030775]\n",
      "Iter:  0  Error:  [36386543.11463701]  J-e: [893087.92567074]  t0:  [-0.04241068]  t1:  [56.66190848]\n",
      "Iter:  100  Error:  [24337561.48792726]  J-e: [704.63595678]  t0:  [-0.21415665]  t1:  [80.49428144]\n",
      "Iter:  200  Error:  [24328054.58891194]  J-e: [0.55611018]  t0:  [-0.22465407]  t1:  [81.16366687]\n",
      "Converged, iterations:  257\n",
      "Computed value\n",
      "Theta0 = [-0.22505359]       Theta1 = [81.18048778]\n",
      "Intercept = 0.0010496294035381298 Slope = 81.1922665739615\n",
      "Error on Slope [0.0117788]\n",
      "Error on Intercept [0.22610322]\n",
      "CPU times: user 40.9 s, sys: 93.5 ms, total: 41 s\n",
      "Wall time: 41.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# We call gradient descent, this gives approximation \n",
    "# of theta0 and theta1\n",
    "# WE CHANGE the value of \"alpha\"\n",
    "#alpha=0.035\n",
    "alpha=0.035\n",
    "theta0, theta1 = Gradient_Method_Prob2_Version_1(alpha, x.copy(), y.copy(), eps, max_iter=1000,\n",
    "                                     steps=100)\n",
    "print(\"Computed value\")\n",
    "print(('Theta0 = %s       Theta1 = %s') %(theta0, theta1)) \n",
    "\n",
    "# We check a good approximation of the \"exact\" value\n",
    "# with scipy linear regression \n",
    "slope, intercept, r_value, p_value, slope_std_error = stats.linregress(x[:,0], y)\n",
    "print(('Intercept = %s Slope = %s') %(intercept, slope))\n",
    "print(\"Error on Slope\", slope-theta1)\n",
    "print(\"Error on Intercept\", intercept-theta0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of input:  1000000\n",
      "Size of Batch:  10000\n",
      "Initial Value of Thetao[0] = -0.03790922883399471\n",
      "Initial Value of Theta1[0] = 55.77006281828385\n",
      "Total Initial Cost 37279631.04031\n",
      "Iter:  0  Error:  36386543.11464  J-e: 893087.92567  t0:  -0.0424106849489516  t1:  56.6619084777314\n",
      "Iter:  100  Error:  24337561.48793  J-e: 704.63596  t0:  -0.21415665156001448  t1:  80.49428143701331\n",
      "Iter:  200  Error:  24328054.58891  J-e: 0.55611  t0:  -0.2246540703253696  t1:  81.16366686632055\n",
      "Converged, iterations:  257\n",
      "Computed value\n",
      "Theta0 = -0.22505111325671467       Theta1 = 81.18039594435312\n",
      "Intercept = 0.0010496294035381298 Slope = 81.1922665739615\n",
      "Error on Slope 0.011870629608381478\n",
      "Error on Intercept 0.22610074266025282\n",
      "CPU times: user 399 ms, sys: 135 ms, total: 534 ms\n",
      "Wall time: 98.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# We call gradient descent, this gives approximation \n",
    "# of theta0 and theta1\n",
    "# WE CHANGE the value of \"alpha\"\n",
    "#alpha=0.035\n",
    "alpha=0.035\n",
    "theta0, theta1,thetas_hist = Gradient_Method_Prob2_Version_2(alpha, x.copy(), y.copy(), eps, max_iter=1000,\n",
    "                                     steps=100)\n",
    "print(\"Computed value\")\n",
    "print(('Theta0 = %s       Theta1 = %s') %(theta0, theta1)) \n",
    "\n",
    "# We check a good approximation of the \"exact\" value\n",
    "# with scipy linear regression \n",
    "slope, intercept, r_value, p_value, slope_std_error = stats.linregress(x[:,0], y)\n",
    "print(('Intercept = %s Slope = %s') %(intercept, slope))\n",
    "print(\"Error on Slope\", slope-theta1)\n",
    "print(\"Error on Intercept\", intercept-theta0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]\n",
      " [2 2]\n",
      " [3 3]\n",
      " [4 4]]\n",
      "[ 6  9 12 15]\n"
     ]
    }
   ],
   "source": [
    "Xx = np.array([[1, 1], [2, 2], [3, 3], [4, 4]])\n",
    "print(Xx)\n",
    "# y = 1 * x_0 + 2 * x_1 + 3\n",
    "yy = np.dot(Xx, np.array([1, 2])) + 3\n",
    "print(yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[1.5 1.5]\n",
      "3.0000000000000018\n",
      "[15.]\n"
     ]
    }
   ],
   "source": [
    "reg = LinearRegression().fit(Xx, yy)\n",
    "print(reg.score(Xx, yy))\n",
    "print(reg.coef_)\n",
    "print(reg.intercept_)\n",
    "print(reg.predict(np.array([[3, 5]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
